{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "r { color: Red }\n",
    "o { color: Orange }\n",
    "g { color: Green }\n",
    "b { color: LightBlue}\n",
    "w { color: White}\n",
    "</style>\n",
    "\n",
    "# **Mobile Robotics Presentation Group 16**\n",
    "EPFL Course MICRO-452, fall 2023\n",
    "- Gilles Regamey 296642\n",
    "- Julien Droulet SCIPER\n",
    "- Tom Rathjen    SCIPER\n",
    "- Aubin Sabatier SCIPER\n",
    "\n",
    "Video of the project:\n",
    "\n",
    "<video width=\"500\" height=\"350\" controls src=\"demo.mp4\">animation</video>\n",
    "\n",
    "## Table of content\n",
    "-   [Choices](#choices)\n",
    "    - environement\n",
    "    - filter\n",
    "    - navigation\n",
    "-   [Modules](#modules)\n",
    "    - Vision\n",
    "    - Global Navigation\n",
    "    - Local Navigation\n",
    "    - Filtering\n",
    "    - Motion Control\n",
    "-   [Demonstration](#demo)\n",
    "    - setup\n",
    "    - loop\n",
    "-   [Results](#results)\n",
    "    - encountered problems\n",
    "    - performance\n",
    "    - improvements\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"choices\"></a>\n",
    "# Choices taken\n",
    "The project called for a environement map to navigate with fixed and un-planned obstacles to a destination point from any point. Fixed obstacles are constant trough the navigation and the robot can't use its sensor to detect them, they are only given by the camera at the start. Un-planned obstacles are detected only by the robot and can move at any time during navigation. \n",
    "\n",
    "## Environement\n",
    "For the sake of simplicity, our map is a white A0 paper sheet with 4 <b>blue</b> box (4x4cm) at the corners. The Fixed obstacles are black pieces of cardboard of any shape. Mobile obstacles are <w>white</w> boxes of 4x4x4 cm 3d printed in PLA to have a more consistent reading with the proximity sensors. The robot is fitted with a paper hat with <r>red</r> circles in a isosceles triangle pattern for position and orientation. The destination is a big green circle 8cm in diameter. The image of the map is reframed to have a resolution of $1 \\frac{px}{mm}$.\n",
    "\n",
    "Below a theoretical map image before processing.\n",
    "\n",
    "<img src=\"src\\Vision\\test_data\\test_map.png\" />\n",
    "\n",
    "The fixed obstacles are abstracted in two ways for two different purposes:\n",
    "- For Global navigation: occupancy grid of constant size.\n",
    "- For Local navigation: polygonal contour approximation.\n",
    "We'll see why in the modules descriptions.  \n",
    "\n",
    "## Filtering\n",
    "For the robot to reach its destination, it needs to know the path it has to take, as well as its current position in the event of new obstacles appearing or the destination changing during the mission. To achieve this, we use an external camera, positioned above the terrain, and speed sensors on the Thymio robot.\n",
    "\n",
    "The camera gives us a good estimate of the robot's position and orientation. But the Thymio must be able to navigate without it if the robot goes out of its field of vision, if an obstacle blocks its field of vision, or if the camera fails to provide usable information due to lack of lighting. In addition, the sampling frequency of the position given by the camera is lower than the frequency of the measurements taken by the speed sensors.\n",
    "\n",
    "However, speed sensor measurements are very noisy. The Kalman Filter algorithm compensates for the drawbacks of each method by merging the position estimate given by the camera with the speed sensor measurements.\n",
    "\n",
    "## Global navigation\n",
    "For the path planning we choose the A\\* Algorithm with the occupancy grid given by the vision module.\n",
    "To have a margin to the obtacle \n",
    "\n",
    "<r>**why A\\* - #[TODO]JULIEN**</r>\n",
    "\n",
    "## Local navigation\n",
    "Mobile and un-planned obstacles have to be detected and avoided. We choose to use a neural network system to be more robust to non standard obstacles. \n",
    "\n",
    "<r>**why neural #[TODO] TOM**</r>\n",
    "\n",
    "<a id=\"modules\"></a>\n",
    "# Modules description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vision**\n",
    "The vision module is responsible for everything concerning the camera and vizualization.\n",
    "Most importantly it has to detect the robot, the obstacles, the destination and the map correctly.\n",
    "Visualization functions have been more of a tool to see and debug outputs of detection.\n",
    "As our map elements are color based, we use the HLS (Hue Luminosity Saturation) encoding of the image to have a better chance at detecting color patches.\n",
    "We use the opencv library extensively because of the blazingly fast parallel computation and all the usefull tools for detection and image processing.\n",
    "\n",
    "### map detection\n",
    "In the setup phase, we need to calibrate the camera to reframe the map in a consitant way to give a baseline for other functions. Opencv has a very usefull function called `warpPerspective()` just for that, we just need to give it the <w>warp matrix</w> and desired output shape. The function `vision.get_warp()` gives us this <w>warp matrix</w> from the image using the algorithm below and with the help of smaller functions called `reorder_border()`, `blob_point_list()` and another opencv function to compute the matrix from reference points `getPerspectiveTransform`. Note that this is not functionnal code, the real function and subsequent vision function can be found in `/src/vision.py`.\n",
    "<small>\n",
    "\n",
    "```python\n",
    "def get_warp(capture_stream,ROI,padding,number_of_samples):\n",
    "    while valid_samples < number_of_samples:\n",
    "        frame = capture_stream.read()\n",
    "        hsl_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HLS_FULL)\n",
    "        nb_points,point_list = blob_point_list(Blue,CORNER_BLOB_FILT,CORNER_BLOB_VAL)\n",
    "        if nb_points == 4:\n",
    "            isvalid,ord_list = reorder_border(point_list)\n",
    "            if isvalid:\n",
    "                corners[:,:,smp] = ord_list\n",
    "                valid_samples += 1\n",
    "    avgCorn = mean(corners) \n",
    "     \n",
    "    return cv2.getPerspectiveTransform(avgCorn,Sheetpts)\n",
    "```\n",
    "\n",
    "</small>\n",
    "\n",
    "The `reorder_border()` function is just ordering the points relative to the center of the polygon they form (above/under,left/right) in order to give a consistant list of point for the averaging process.\n",
    "\n",
    "Note that no action is taken for tangential and radial distortions. Opencv provides function for camera calibration and undistortion methods that need a calibration pattern (Charuco boards). We tried to use it but coulnd't get consitant camera matrix, it just made things worse most of the time so we decided to drop the idea knowing that robot position detection was good enough at our scale.\n",
    "\n",
    "### Obstacle detection and abstraction\n",
    "\n",
    "For the grid map of the obstacles we just apply a threshold to a scaled down filtered grayscale image of the map using opencv functions. You can also add a safe radius around obstacles using dilatation. \n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "def get_grid_fixed_map(frame,shape,tresh=50,kernsz=5,robrad=80):\n",
    "    kernel = np.ones((kernsz,kernsz),np.uint8)\n",
    "    pxmap = cv2.inRange(frame,(0,0,0),(tresh,tresh,tresh))\n",
    "    pxmap = cv2.morphologyEx(pxmap,cv2.MORPH_OPEN,kernel)\n",
    "    if robrad != 0:\n",
    "        safecircle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(robrad,robrad))\n",
    "        pxmap = cv2.dilate(pxmap,safecircle)\n",
    "    temp = cv2.resize(pxmap, shape, interpolation=cv2.INTER_LINEAR)\n",
    "    _,output = cv2.threshold(temp,10,1,type=cv2.THRESH_BINARY)\n",
    "```\n",
    "</small>\n",
    "\n",
    "For the polygonal abstraction of the obstacles, we use opencv `findContours()` and `approxPolyDP()` to get an array of obstacles described as arrays of corner points.\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "def get_obstacles(frame,tresh=50,eps=10,robrad=0):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    pxmap = cv2.inRange(frame,(0,0,0),(tresh,tresh,tresh))\n",
    "    pxmap = cv2.morphologyEx(pxmap,cv2.MORPH_OPEN,kernel)\n",
    "    if robrad != 0:\n",
    "        safecircle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(robrad,robrad))\n",
    "        pxmap = cv2.dilate(pxmap,safecircle)\n",
    "    contp,hier =  cv2.findContours(pxmap,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    obstacles = []\n",
    "    for cont in contp:\n",
    "        cont = np.array(cont,dtype=np.float32).reshape((-1,2))\n",
    "        scont = cv2.approxPolyDP(cont,eps,True)\n",
    "        obstacles.append(scont)\n",
    "    return obstacles\n",
    "```\n",
    "</small>\n",
    "\n",
    "### Robot detection of position and orientation\n",
    "\n",
    "This function is a bit more involved. The goal was to detect quickly the position between the wheels and the orientation vector. If we have a pattern with no rotational symetry, in our case a triangle with two side equal and longer than the last, we can get the points at the corners easily and compute everything with them. The hard part is knowing which point is which. Here is the algorithm:\n",
    "<small>\n",
    "```python\n",
    "nbdot,points = filter_for_dots(hlsframe)\n",
    "if nbdot >= 3:\n",
    "    for p1 in range(0,nbdot-1):\n",
    "        for p2 in range(p1+1,nbdot):\n",
    "            pairwise_dist = dist(p1,p2)\n",
    "else: \n",
    "    return False\n",
    "\n",
    "max = argmax(pairwise_dist)\n",
    "long = logic_array(abs(max-pairwise_dist)<epsilon)\n",
    "short = logic_array(abs(max*ratio-pairwise_dist)<epsilon)\n",
    "if(sum(short)==1 and sum(long)==2): \n",
    "    pairAB = where(shortidx == 1)\n",
    "    Cidx = where((sum(longidx+longidx^t,1))==2)\n",
    "    center = np.array([int((ptlist[pairAB[0][0]][0] + ptlist[pairAB[1][0]][0])/2),\n",
    "                        int((ptlist[pairAB[0][0]][1] + ptlist[pairAB[1][0]][1])/2)])\n",
    "    scale = d_table[pairAB]/4\n",
    "    ptC = np.array([int(ptlist[Cidx[0][0]][0]),int(ptlist[Cidx[0][0]][1])])\n",
    "    dirvect = ptC - center\n",
    "    orient = -np.arctan2(dirvect[1],dirvect[0])\n",
    "    return True,center,orient,scale\n",
    "else:\n",
    "    return False\n",
    "\n",
    "```\n",
    "<small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "The Extended Kalman Filter is an extension of the Kalman Filter for non-linear systems. The filter estimates the value of the vector (x, y, \\theta, v, w), whose horizontal and vertical position depends on sinusoidal functions. To calculate the evolution of the position, the time interval between two measurements must take into account the time it takes for the program to be run. The time variable \"dt\" is initialized at the start of the program, then re-initialized at the end of a Kalman filter estimate. It must also be re-initialized if the thymio is moved during the mission.\n",
    "\n",
    "Translational speed and angular velocity were calculated from sensor measurements on each wheel in the \"speed_estimation\" function. As the sensor measurements were not in standardized units, conversion was performed using calibration constants.\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "def speed_estimation(left_speed, right_speed):\n",
    "    speed_measured = (right_speed + left_speed) / 2\n",
    "    speed = (speed_measured * REAL_THYMIO_SPEED) / COMMAND_MOTOR_FOR_CALIBRATION\n",
    "    angular_speed_measured = (right_speed - left_speed) / 2\n",
    "    angular_speed = (angular_speed_measured * REAL_THYMIO_ANGULAR_SPEED) / COMMAND_MOTOR_FOR_CALIBRATION\n",
    "\n",
    "    return speed, angular_speed\n",
    "```\n",
    "</small>\n",
    "\n",
    "Here is how the function is initialized:\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "speed, angular_speed = speed_estimation(left_speed, right_speed)\n",
    "\n",
    "dt = time.time() - start_time \n",
    "state_estimation, P_estimation = ex_kalman_filter(float speed, float angular_speed, bool camera_got_pos, array position_from_camera,\n",
    "                                                  array state_estimation, array P_estimation, float dt)\n",
    "start_time = time.time()\n",
    "```\n",
    "</small>\n",
    "\n",
    "To estimate the robot's next state, the filter first predicts values based on the previous state, according to the following equations:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_{k} + v_{k} * cos(\\theta _{k}) * dt\n",
    "$$\n",
    "$$\n",
    "y_{k+1} = y_{k} + v_{k} * sin(\\theta _{k}) * dt\n",
    "$$\n",
    "$$\n",
    "\\theta _{k+1} = \\theta _{k} + v_{k} * cos(\\theta _{k}) * dt\n",
    "$$\n",
    "$$\n",
    "v_{k+1} = v_{k}\n",
    "$$\n",
    "$$\n",
    "w_{k+1} = w_{k}\n",
    "$$\n",
    "\n",
    "Which are implemented as follows:\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "## Prediciton Step, through the previous estimation\n",
    "A = np.array([[1, 0, 0, np.cos(theta).item() * dt, 0],\n",
    "              [0, 1, 0, np.sin(theta).item() * dt, 0],\n",
    "              [0, 0, 1, 0, dt],\n",
    "              [0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 0, 1]])\n",
    "predicted_state_estimation = np.dot(A, previous_state_estimation)\n",
    "```\n",
    "</small>\n",
    "\n",
    "After measuring the standard deviation of velocity and angular rate measurements during a typical mission, it was possible to approximate their error. Assuming that half the variation is caused by the measurements and the other half by perturbations of the states by external factors (terrain, wheel slippage, etc.).\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "STD_SPEED = 3 # mm^2/s^2\n",
    "STD_ANGULAR_SPEED = 0.04 # rad^2/s^2\n",
    "\n",
    "q_nu_translation = STD_SPEED / 2 # variance on speed state\n",
    "r_nu_translation = STD_SPEED / 2 # variance on speed measurement\n",
    "q_nu_rotation = STD_ANGULAR_SPEED / 2 # variance on angular speed state\n",
    "r_nu_rotation = STD_ANGULAR_SPEED / 2 # variance on angular speed measurement\n",
    "```\n",
    "\n",
    "</small>\n",
    "\n",
    "Uncertainty in state prediction is only influenced by velocity and angular speed, as these are included in the equations for position estimation. The process noise covariance matrix for prediction is as follows:\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "Q = np.array([[0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, q_nu_translation, 0],\n",
    "              [0, 0, 0, 0, q_nu_rotation]])\n",
    "\n",
    "predicted_state_estimation_jacobian=\n",
    "np.array([[1,0,-previous_state_estimation[3].item()*np.sin(theta).item()*dt, np.cos(theta).item()*dt,0], \n",
    "                                              [0, 1,previous_state_estimation[3].item()*np.cos(theta).item()*dt,np.sin(theta).item()*dt,0],\n",
    "                                              [0, 0, 1, 0, dt],\n",
    "                                              [0, 0, 0, 1, 0],\n",
    "                                              [0, 0, 0, 0, 1]])\n",
    "predicted_covariance_estimation = np.dot(predicted_state_estimation_jacobian,\n",
    "                                         np.dot(previous_covariance_estimation, predicted_state_estimation_jacobian.T)) + Q\n",
    "```\n",
    "\n",
    "</small>\n",
    "\n",
    "In a second step, the values are updated by the sensor measurements and the external camera estimation.\n",
    "To update the values, the algorithm takes into account the uncertainty of each measurement through the R matrix. After representing the variation in the camera's position estimation, the horizontal and vertical error (RP) is estimated at +/- 1cm, and the orientation error (RP_ANGLE) at ≈1.5 degrees. Here's the code to update the prediction:\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "## Update Step      \n",
    "    if cam_got_pos and position_from_camera is not None:\n",
    "        # camera position is available\n",
    "        y = np.array([[position_from_camera[0]], [position_from_camera[1]], [position_from_camera[2]], [speed], [angular_speed]])\n",
    "        H = np.identity(5)\n",
    "        R = np.array([[RP, 0, 0, 0, 0],\n",
    "                      [0, RP, 0, 0, 0],\n",
    "                      [0, 0, RP_ANGLE, 0, 0],\n",
    "                      [0, 0, 0, r_nu_translation, 0],\n",
    "                      [0, 0, 0, 0, r_nu_rotation]]) # process noise covariance matrix\n",
    "    else:\n",
    "        # no transition, use only the speed\n",
    "        y = np.array([[speed], [angular_speed]])\n",
    "        H = np.array([[0, 0, 0, 1, 0], [0, 0, 0, 0, 1]])\n",
    "        R = np.array([[r_nu_translation, 0], [0, r_nu_rotation]]) # process noise covariance matrix\n",
    "\n",
    "    # measurement residual\n",
    "    i = y - np.dot(H, predicted_state_estimation)\n",
    "    # measurement prediction covariance\n",
    "    S = np.dot(H, np.dot(predicted_covariance_estimation, H.T)) + R\n",
    "    # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "    K = np.dot(predicted_covariance_estimation, np.dot(H.T, np.linalg.inv(S)))\n",
    "\n",
    "    # Updated state and covariance estimate\n",
    "    state_estimation = predicted_state_estimation + np.dot(K, i)    \n",
    "    P_estimation = np.dot((np.identity(5) - np.dot(K, H)), predicted_covariance_estimation)\n",
    "```\n",
    "\n",
    "</small>\n",
    "The function returns the estimated position and covariance, which will be used to calculate the prediction for the next estimation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"demo\"></a>\n",
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import opencv_jupyter_ui as jcv2\n",
    "\n",
    "## Custom Libraries \n",
    "from src.Motion_Control import thymio as th\n",
    "from src.Global_Nav import helpers_global as gn\n",
    "from src.Vision import vision as vs\n",
    "from src.Local_Nav import psymap as pm  \n",
    "from src.Local_Nav import local_navigation as ln\n",
    "from src.Filtering import filtering\n",
    "from src.Motion_Control import PID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constant \n",
    "REFRAME = True \n",
    "TS =0.01\n",
    "EPSILON_ANGLE= np.pi/10\n",
    "VISUALIZE = True\n",
    "MAP_SHAPE_MM = (1000,700)\n",
    "MAP_SHAPE_CELL = (50,35)\n",
    "ROBROAD = 80\n",
    "SIMPLIFY = 0.8\n",
    "SAVE_VIDEO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node() #_ = protected #__ = private = shouldn't access node outside of the class\n",
    "await node.lock()\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if SAVE_VIDEO:\n",
    "    videosaver = cv2.VideoWriter('VideoG16.avi',  cv2.VideoWriter_fourcc(*'MJPG'), 10, MAP_SHAPE_MM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "if REFRAME:\n",
    "    Tmap = vs.get_warp(cap,MAP_SHAPE_MM,20,10)\n",
    "\n",
    "ret,frame = cap.read()\n",
    "if ret:\n",
    "    if REFRAME:\n",
    "        frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "    fmap = vs.get_grid_fixed_map(frame,MAP_SHAPE_CELL)\n",
    "    obscont = vs.get_obstacles(frame)\n",
    "    print(\"Searching for destination...\")\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        if ret:\n",
    "            if REFRAME:\n",
    "                frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "            ret,destmm = vs.get_destination(frame)\n",
    "            if ret:\n",
    "                dest = gn.convert_to_idx([coord / 10.0 for coord in destmm],2)\n",
    "                dest[1]= 35-dest[1]\n",
    "                dest = tuple(dest)\n",
    "                break\n",
    "            else:\n",
    "                print(\"no destination !\",end='\\r')\n",
    "        else:\n",
    "            print(\"No camera !\")\n",
    "            break\n",
    "    print(\"Found destination Point at {} [mm] {} [cells]\".format(destmm,dest))\n",
    "    print(\"Searching for Robot...\")\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        if ret:\n",
    "            if REFRAME:\n",
    "                frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "            hls_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HLS_FULL) \n",
    "            ret,robpos,orient,pxpcm = vs.get_Robot_position_orientation(hls_frame)\n",
    "            if ret:\n",
    "                print(\"Robot found at {} [mm], {} [rad]\".format(robpos,orient))\n",
    "                break\n",
    "        else:\n",
    "            print(\"No camera !\")\n",
    "            break\n",
    "\n",
    "start = gn.convert_to_idx(robpos,20)\n",
    "start[1]= MAP_SHAPE_CELL[1]-start[1]\n",
    "start = tuple(start)\n",
    "path = gn.global_final(fmap,start,dest, \"8N\", VISUALIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Main boucle with Kalman Aubin\n",
    "\n",
    "local_obstacle = False\n",
    "counter=0\n",
    "cell_pos_history= []\n",
    "kidnapping_state = False\n",
    "check=[]\n",
    "check_point_prev=np.array([0,0])\n",
    "Kp = 50\n",
    "spdPID = 100\n",
    "start_time = time.time()\n",
    "\n",
    "state_estimation_prev= np.array([[robpos[0]],[robpos[1]],[orient],[0],[0]])\n",
    "P_estimation_prev =  np.diag([100, 100, 0.75, 10, 0.75])\n",
    "\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    if ret:\n",
    "\n",
    "        if REFRAME:\n",
    "            frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "\n",
    "        HLS = cv2.cvtColor(frame, cv2.COLOR_BGR2HLS_FULL)\n",
    "\n",
    "        camgotpos ,robpos,orient, scale = vs.get_Robot_position_orientation(HLS, 5)\n",
    "        if camgotpos : \n",
    "\n",
    "            robpos[1] = MAP_SHAPE_MM[1]- robpos[1]\n",
    "            orient2 = orient\n",
    "            if orient <0:\n",
    "                orient = orient +2*np.pi\n",
    "            x_est_cam = np.array([robpos[0], robpos[1], orient])\n",
    "            cell_pos_cam = np.array([x_est_cam[0]/20, x_est_cam[1]/20, orient2])\n",
    "            cell_pos_history.append(cell_pos_cam)\n",
    "\n",
    "\n",
    "        ground_values = await th.get_proximity_ground_values(client)\n",
    "        if(ground_values[0]<300 or ground_values[1]< 300):\n",
    "            print('Kidnapping detected')\n",
    "            await th.stop_motor(node)\n",
    "            kidnapping_state= True\n",
    "\n",
    "        if ground_values[0]>300 and ground_values[1]>300 and kidnapping_state and camgotpos:\n",
    "\n",
    "            kidnapping_state = False\n",
    "            start = gn.convert_to_idx(robpos,20)\n",
    "            start = tuple(start)\n",
    "            path = gn.global_final(fmap,start,dest, \"8N\", VISUALIZE)\n",
    "            state_estimation_prev = np.array([[robpos[0]],[robpos[1]], [orient], [0], [0]])\n",
    "            P_estimation_prev =  np.diag([100, 100, 0.75, 10, 0.75])\n",
    "            counter =0\n",
    "            check_point_prev=np.array([0,0])\n",
    "            start_time = time.time()\n",
    "     \n",
    "        state_estimation, P_estimation, speed, angular_speed, start_time, angle = await filtering.get_position(state_estimation_prev, P_estimation_prev, start_time,camgotpos,x_est_cam, node )\n",
    "        state_estimation_prev = state_estimation\n",
    "        P_estimation_prev = P_estimation\n",
    "\n",
    "        position = np.array([state_estimation[0].item(), state_estimation[1].item()])\n",
    "        theta = angle\n",
    "        position = position / 20.0\n",
    "        position_array = np.array(position)\n",
    "\n",
    "        state_history = np.array([position_array[0], position_array[1], theta[0]])\n",
    "        check.append(state_history)\n",
    "\n",
    "        check_point, counter = gn.next_checkpoint2(path, position, counter,local_obstacle)\n",
    "            \n",
    "        if not kidnapping_state:\n",
    "            if np.any(check_point_prev != check_point):\n",
    "                print(f\"robot at {position}, grid coord {gn.convert_to_idx(position, 1)} next checkpoint at{check_point}\")\n",
    "                check_point_prev = check_point\n",
    "                \n",
    "            if abs(position[0]-path[-1][0])<1 and abs(position[1]-path[-1][1])<1:\n",
    "                await th.stop_motor(node)\n",
    "                break\n",
    "\n",
    "            \n",
    "            angle_error=  theta-th.compute_angle(gn.convert_to_idx(position,1), path[counter])\n",
    "            if angle_error > np.pi :\n",
    "                angle_error = angle_error-2*np.pi\n",
    "            if angle_error < -np.pi:\n",
    "                angle_error = angle_error+ 2*np.pi\n",
    "\n",
    "            \n",
    "            capthall = pm.hallucinate_map([position[0],position[1],(-orient)],obscont)\n",
    "            sens = await th.get_proximity_values(client)\n",
    "            if (sum(sens[i] > 1000 for i in range(0, 5)) > 0):\n",
    "                local_obstacle = True\n",
    "\n",
    "            if(local_obstacle):\n",
    "                print(\"Entering Local navigation mode\")\n",
    "                await ln.local_navigation(client,node,[position[0],position[1],(-orient)],obscont)\n",
    "                \n",
    "                if(not sum(sens[i] > 1000 for i in range(0, 5)) > 0):\n",
    "                    await th.motorset(node,100,100)\n",
    "                    time.sleep(1.5)\n",
    "                    local_obstacle = False\n",
    "            #motor control\n",
    "            else :\n",
    "                if(angle_error>EPSILON_ANGLE):\n",
    "                    await th.motorset(node,70,-70)\n",
    "                elif (angle_error<-EPSILON_ANGLE):\n",
    "                    await th.motorset(node,-70,70)\n",
    "                else:\n",
    "                    speed_l, speed_r = PID.PIDController(Kp,spdPID, angle_error)\n",
    "                    await th.motorset(node,speed_l,speed_r)\n",
    "                    \n",
    "\n",
    "        if VISUALIZE:\n",
    "            vizu = vs.visualizer(HLS)\n",
    "            omap =vs.grid_fixedmap_visualizer(fmap.transpose(),MAP_SHAPE_MM)\n",
    "            obsimg = cv2.merge([omap,omap,omap])\n",
    "            vizu = cv2.bitwise_or(vizu,obsimg)\n",
    "            vizu = vs.draw_obstacles_poly(vizu,obscont,(255,255,0),2)\n",
    "            vizu = cv2.circle(vizu,destmm,20,(50,25,100),4)\n",
    "            vizu = cv2.addWeighted(vizu,0.5,frame,0.5,0)\n",
    "            vizu = vs.show_path(vizu,path,20,10)\n",
    "            vizu = vs.show_Kalman(vizu,state_estimation,P_estimation,10)\n",
    "            robpos[1] = MAP_SHAPE_MM[1] - robpos[1]\n",
    "            vizu = vs.paint_robot(vizu,(0,0,200),robpos,orient,pxpcm)\n",
    "            vizu = pm.hallucinate_map([robpos[0],robpos[1],(-orient)],obscont,vizu)\n",
    "            videosaver.write(vizu) \n",
    "            jcv2.imshow(\"Map\",vizu)\n",
    "            if jcv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                await th.stop_motor(node)\n",
    "                break\n",
    "        else:\n",
    "            print(\"###################################################\",end='\\r')\n",
    "            print(\"pos: {},{:.2f} dest: {} fhal: {}\".format(robpos,orient,destmm,capthall),end='\\r')\n",
    "    \n",
    "    else :\n",
    "        print(\"error : camera failure.\")\n",
    "        break\n",
    "\n",
    "videosaver.release()\n",
    "jcv2.destroyAllWindows()\n",
    "await th.stop_motor(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_values = [coord[0] for coord in check]\n",
    "y_values = [coord[1] for coord in check]\n",
    "x_path =   [coord[0] for coord in path]\n",
    "y_path =  [coord[1] for coord in path]\n",
    "\n",
    "# Tracer le graphique x en fonction de y\n",
    "plt.plot(x_values, y_values, marker='.', linestyle='-')\n",
    "plt.plot(x_path, y_path, marker ='o', color = 'red')\n",
    "plt.axis('equal')\n",
    "plt.title('Historique de position')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await th.stop_motor(node)\n",
    "node.unlock()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MobileRobotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "r { color: Red }\n",
    "o { color: Orange }\n",
    "g { color: Green }\n",
    "b { color: LightBlue}\n",
    "w { color: White}\n",
    "</style>\n",
    "\n",
    "# **Mobile Robotics Presentation Group 16**\n",
    "EPFL Course MICRO-452, fall 2023\n",
    "- Gilles Regamey 296642\n",
    "- Julien Droulet 328082\n",
    "- Tom Rathjens   325155\n",
    "- Aubin Sabatier 310741\n",
    "\n",
    "Video of the project:\n",
    "\n",
    "<video width=\"500\" height=\"350\" controls src=\"demo.mp4\">animation</video>\n",
    "\n",
    "## Table of content\n",
    "-   [Choices](#choices)\n",
    "    - environement\n",
    "    - filter\n",
    "    - navigation\n",
    "-   [Modules](#modules)\n",
    "    - [Vision](#vision)\n",
    "    - [Filtering](#filtering)\n",
    "    - [Global Navigation](#globnav)\n",
    "    - [Local Navigation](#locnav)\n",
    "    - [Motion Control](#motion)\n",
    "-   [Demonstration](#demo)\n",
    "    - setup\n",
    "    - loop\n",
    "-   [Results](#results)\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"choices\"></a>\n",
    "# Choices taken\n",
    "The project called for a environement map to navigate with fixed and un-planned obstacles to a destination point from any point. Fixed obstacles are constant trough the navigation and the robot can't use its sensor to detect them, they are only given by the camera at the start. Un-planned obstacles are detected only by the robot and can move at any time during navigation. \n",
    "\n",
    "## Environement\n",
    "For the sake of simplicity, our map is a white A0 paper sheet with 4 <b>blue</b> box (4x4cm) at the corners. The Fixed obstacles are black pieces of cardboard of any shape. Mobile obstacles are <w>white</w> boxes of 4x4x4 cm 3d printed in PLA to have a more consistent reading with the proximity sensors. The robot is fitted with a paper hat with <r>red</r> circles in a isosceles triangle pattern for position and orientation. The destination is a big green circle 8cm in diameter. The image of the map is reframed to have a resolution of $1 \\frac{px}{mm}$.\n",
    "\n",
    "Below a theoretical map image before processing.\n",
    "\n",
    "<img src=\"src\\Vision\\test_data\\test_map.png\" />\n",
    "\n",
    "The fixed obstacles are abstracted in two ways for two different purposes:\n",
    "- For Global navigation: occupancy grid of constant size.\n",
    "- For Local navigation: polygonal contour approximation.\n",
    "We'll see why in the modules descriptions.  \n",
    "\n",
    "## Filtering\n",
    "For the robot to reach its destination, it needs to know the path it has to take, as well as its current position in the event of new obstacles appearing or the destination changing during the mission. To achieve this, we use an external camera, positioned above the terrain, and speed sensors on the Thymio robot.\n",
    "\n",
    "The camera gives us a good estimate of the robot's position and orientation. But the Thymio must be able to navigate without it if the robot goes out of its field of vision, if an obstacle blocks its field of vision, or if the camera fails to provide usable information due to lack of lighting. In addition, the sampling frequency of the position given by the camera is lower than the frequency of the measurements taken by the speed sensors.\n",
    "\n",
    "However, speed sensor measurements are very noisy. The Kalman Filter algorithm compensates for the drawbacks of each method by merging the position estimate given by the camera with the speed sensor measurements.\n",
    "\n",
    "## Global navigation\n",
    "\n",
    "For the path planning we choose the A\\* Algorithm with the occupancy grid given by the vision module. We choosed to use A\\* as it is the simplest algorithm to have the optimal path\n",
    "To have a margin to the obtacle we modified the cost of the cell by multipliying it by 2 if the cell is in a radius of 3 cells from an obstacle and by 1.5 if the cell is in a radius of 5 cell from the obstacle.\n",
    "\n",
    "We also used the Douglas Peucker algorithm to simplify the path given by A\\*, and have a smoother trajectory. \n",
    "Finally, we decided to use a path containing only the points where the direction is changing.\n",
    "\n",
    "## Local navigation\n",
    "\n",
    "Mobile and un-planned obstacles must be detected and avoided. This is done in `local_navigation()` It's called and executed in the main, when the front proximity sensors detect an obstacle `(prox value>500)`.\n",
    "\n",
    "We choose to use a neural network system to be more robust to non-standard obstacles. In our testing, this system responded better to various obstacle shapes and orientations when compared to our second algorithm, the wall-following prototype. The limitations of the wall-following algorithm are mostly due to the absence of lateral sensors on the Thymio robot. The ANN is inspired by the course of Pr.Mondada.\n",
    "\n",
    "The neural network system utilizes two sets of five weights to determine the target speeds for the left and right motors based on readings from each proximity sensor. To incorporate a global perspective of obstacles into the artificial neural network, we hallucinate the fixed obstacles with a custom function. This virtual sensor simulates obstacles by assigning proximity sensor values based on the robot's position and the global obstacle positions. For more details, checkout out Vision’s documentation.\n",
    "The weights are tuned by repetitive testing.\n",
    "We also implemented the `front_obst_avoidance_seq()` function. It's executed in the main program when the front proximity sensor detects an obstacle within 2 cm of the robot. This may occur if local obstacles are placed too late in front of the robot, leaving insufficient time for the ANN to navigate around them. In this case, the robot stops its motors, executes a $\\pi$ turn, moves forward a specified distance, and then performs a -$\\pi$ turn to resume its trip.\n",
    "\n",
    "<a id=\"modules\"></a>\n",
    "# **Modules description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vision\"></a>\n",
    "## **Vision**\n",
    "\n",
    "The vision module is responsible for everything concerning the camera and vizualization.\n",
    "Most importantly it has to detect the robot, the obstacles, the destination and the map correctly.\n",
    "Visualization functions have been more of a tool to see and debug outputs of detection.\n",
    "As our map elements are color based, we use the HLS (Hue Luminosity Saturation) encoding of the image to have a better chance at detecting color patches.\n",
    "We use the opencv library extensively because of the blazingly fast parallel computation and all the usefull tools for detection and image processing.\n",
    "\n",
    "### map detection\n",
    "In the setup phase, we need to calibrate the camera to reframe the map in a consitant way to give a baseline for other functions. Opencv has a very usefull function called `warpPerspective()` just for that, we just need to give it the <w>warp matrix</w> and desired output shape. The function `vision.get_warp()` gives us this <w>warp matrix</w> from the image using the algorithm below and with the help of smaller functions called `reorder_border()`, `blob_point_list()` and another opencv function to compute the matrix from reference points `getPerspectiveTransform`. Note that this is not functionnal code, the real function and subsequent vision function can be found in `/src/vision.py`.\n",
    "<small>\n",
    "\n",
    "```python\n",
    "def get_warp(capture_stream,ROI,padding,number_of_samples):\n",
    "    while valid_samples < number_of_samples:\n",
    "        frame = capture_stream.read()\n",
    "        hsl_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HLS_FULL)\n",
    "        nb_points,point_list = blob_point_list(Blue,CORNER_BLOB_FILT,CORNER_BLOB_VAL)\n",
    "        if nb_points == 4:\n",
    "            isvalid,ord_list = reorder_border(point_list)\n",
    "            if isvalid:\n",
    "                corners[:,:,smp] = ord_list\n",
    "                valid_samples += 1\n",
    "    avgCorn = mean(corners) \n",
    "     \n",
    "    return cv2.getPerspectiveTransform(avgCorn,Sheetpts)\n",
    "```\n",
    "\n",
    "</small>\n",
    "\n",
    "The `reorder_border()` function is just ordering the points relative to the center of the polygon they form (above/under,left/right) in order to give a consistant list of point for the averaging process.\n",
    "\n",
    "Note that no action is taken for tangential and radial distortions. Opencv provides function for camera calibration and undistortion methods that need a calibration pattern (Charuco boards). We tried to use it but coulnd't get consitant camera matrix, it just made things worse most of the time so we decided to drop the idea knowing that robot position detection was good enough at our scale.\n",
    "\n",
    "### Obstacle detection and abstraction\n",
    "\n",
    "For the grid map of the obstacles we just apply a threshold to a scaled down filtered grayscale image of the map using opencv functions. You can also add a safe radius around obstacles using dilatation. \n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "def get_grid_fixed_map(frame,shape,tresh=50,kernsz=5,robrad=80):\n",
    "    kernel = np.ones((kernsz,kernsz),np.uint8)\n",
    "    pxmap = cv2.inRange(frame,(0,0,0),(tresh,tresh,tresh))\n",
    "    pxmap = cv2.morphologyEx(pxmap,cv2.MORPH_OPEN,kernel)\n",
    "    if robrad != 0:\n",
    "        safecircle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(robrad,robrad))\n",
    "        pxmap = cv2.dilate(pxmap,safecircle)\n",
    "    temp = cv2.resize(pxmap, shape, interpolation=cv2.INTER_LINEAR)\n",
    "    _,output = cv2.threshold(temp,10,1,type=cv2.THRESH_BINARY)\n",
    "```\n",
    "</small>\n",
    "\n",
    "For the polygonal abstraction of the obstacles, we use opencv `findContours()` and `approxPolyDP()` to get an array of obstacles described as arrays of corner points.\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "def get_obstacles(frame,tresh=50,eps=10,robrad=0):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    pxmap = cv2.inRange(frame,(0,0,0),(tresh,tresh,tresh))\n",
    "    pxmap = cv2.morphologyEx(pxmap,cv2.MORPH_OPEN,kernel)\n",
    "    if robrad != 0:\n",
    "        safecircle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(robrad,robrad))\n",
    "        pxmap = cv2.dilate(pxmap,safecircle)\n",
    "    contp,hier =  cv2.findContours(pxmap,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    obstacles = []\n",
    "    for cont in contp:\n",
    "        cont = np.array(cont,dtype=np.float32).reshape((-1,2))\n",
    "        scont = cv2.approxPolyDP(cont,eps,True)\n",
    "        obstacles.append(scont)\n",
    "    return obstacles\n",
    "```\n",
    "</small>\n",
    "\n",
    "### Robot detection of position and orientation\n",
    "\n",
    "This function is a bit more involved. The goal was to detect quickly the position between the wheels and the orientation vector. If we have a pattern with no rotational symetry, in our case a triangle with two side equal and longer than the last, we can get the points at the corners easily and compute everything with them. The hard part is knowing which point is which. Here is the algorithm:\n",
    "<small>\n",
    "```python\n",
    "nbdot,points = filter_for_dots(hlsframe)\n",
    "if nbdot >= 3:\n",
    "    for p1 in range(0,nbdot-1):\n",
    "        for p2 in range(p1+1,nbdot):\n",
    "            pairwise_dist = dist(p1,p2)\n",
    "else: \n",
    "    return False\n",
    "\n",
    "max = argmax(pairwise_dist)\n",
    "long = logic_array(abs(max-pairwise_dist)<epsilon)\n",
    "short = logic_array(abs(max*ratio-pairwise_dist)<epsilon)\n",
    "if(sum(short)==1 and sum(long)==2): \n",
    "    pairAB = where(shortidx == 1)\n",
    "    Cidx = where((sum(longidx+longidx^t,1))==2)\n",
    "    center = np.array([int((ptlist[pairAB[0][0]][0] + ptlist[pairAB[1][0]][0])/2),\n",
    "                        int((ptlist[pairAB[0][0]][1] + ptlist[pairAB[1][0]][1])/2)])\n",
    "    scale = d_table[pairAB]/4\n",
    "    ptC = np.array([int(ptlist[Cidx[0][0]][0]),int(ptlist[Cidx[0][0]][1])])\n",
    "    dirvect = ptC - center\n",
    "    orient = -np.arctan2(dirvect[1],dirvect[0])\n",
    "    return True,center,orient,scale\n",
    "else:\n",
    "    return False\n",
    "\n",
    "```\n",
    "<small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filtering\"></a>\n",
    "## **Filtering**\n",
    "\n",
    "The Extended Kalman Filter is an extension of the Kalman Filter for non-linear systems. The filter estimates the value of the vector (x, y, \\theta, v, w), whose horizontal and vertical position depends on sinusoidal functions. To calculate the evolution of the position, the time interval between two measurements must take into account the time it takes for the program to be run. The time variable \"dt\" is initialized at the start of the program, then re-initialized at the end of a Kalman filter estimate. It must also be re-initialized if the thymio is moved during the mission.\n",
    "\n",
    "Translational speed and angular velocity were calculated from sensor measurements on each wheel in the \"speed_estimation\" function. As the sensor measurements were not in standardized units, conversion was performed using calibration constants.\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "def speed_estimation(left_speed, right_speed):\n",
    "    speed_measured = (right_speed + left_speed) / 2\n",
    "    speed = (speed_measured * REAL_THYMIO_SPEED) / COMMAND_MOTOR_FOR_CALIBRATION\n",
    "    angular_speed_measured = (right_speed - left_speed) / 2\n",
    "    angular_speed = (angular_speed_measured * REAL_THYMIO_ANGULAR_SPEED) / COMMAND_MOTOR_FOR_CALIBRATION\n",
    "\n",
    "    return speed, angular_speed\n",
    "```\n",
    "</small>\n",
    "\n",
    "Here is how the function is initialized:\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "speed, angular_speed = speed_estimation(left_speed, right_speed)\n",
    "\n",
    "dt = time.time() - start_time \n",
    "state_estimation, P_estimation = ex_kalman_filter(float speed, float angular_speed, bool camera_got_pos, array position_from_camera,\n",
    "                                                  array state_estimation, array P_estimation, float dt)\n",
    "start_time = time.time()\n",
    "```\n",
    "</small>\n",
    "\n",
    "To estimate the robot's next state, the filter first predicts values based on the previous state, according to the following equations:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_{k} + v_{k} * cos(\\theta _{k}) * dt\n",
    "$$\n",
    "$$\n",
    "y_{k+1} = y_{k} + v_{k} * sin(\\theta _{k}) * dt\n",
    "$$\n",
    "$$\n",
    "\\theta _{k+1} = \\theta _{k} + v_{k} * cos(\\theta _{k}) * dt\n",
    "$$\n",
    "$$\n",
    "v_{k+1} = v_{k}\n",
    "$$\n",
    "$$\n",
    "w_{k+1} = w_{k}\n",
    "$$\n",
    "\n",
    "Which are implemented as follows:\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "## Prediciton Step, through the previous estimation\n",
    "A = np.array([[1, 0, 0, np.cos(theta).item() * dt, 0],\n",
    "              [0, 1, 0, np.sin(theta).item() * dt, 0],\n",
    "              [0, 0, 1, 0, dt],\n",
    "              [0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 0, 1]])\n",
    "predicted_state_estimation = np.dot(A, previous_state_estimation)\n",
    "```\n",
    "</small>\n",
    "\n",
    "After measuring the standard deviation of velocity and angular rate measurements during a typical mission, it was possible to approximate their error. Assuming that half the variation is caused by the measurements and the other half by perturbations of the states by external factors (terrain, wheel slippage, etc.).\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "STD_SPEED = 3 # mm^2/s^2\n",
    "STD_ANGULAR_SPEED = 0.04 # rad^2/s^2\n",
    "\n",
    "q_nu_translation = STD_SPEED / 2 # variance on speed state\n",
    "r_nu_translation = STD_SPEED / 2 # variance on speed measurement\n",
    "q_nu_rotation = STD_ANGULAR_SPEED / 2 # variance on angular speed state\n",
    "r_nu_rotation = STD_ANGULAR_SPEED / 2 # variance on angular speed measurement\n",
    "```\n",
    "\n",
    "</small>\n",
    "\n",
    "Uncertainty in state prediction is only influenced by velocity and angular speed, as these are included in the equations for position estimation. The process noise covariance matrix for prediction is as follows:\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "Q = np.array([[0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, q_nu_translation, 0],\n",
    "              [0, 0, 0, 0, q_nu_rotation]])\n",
    "\n",
    "predicted_state_estimation_jacobian=\n",
    "np.array([[1,0,-previous_state_estimation[3].item()*np.sin(theta).item()*dt, np.cos(theta).item()*dt,0], \n",
    "                                              [0, 1,previous_state_estimation[3].item()*np.cos(theta).item()*dt,np.sin(theta).item()*dt,0],\n",
    "                                              [0, 0, 1, 0, dt],\n",
    "                                              [0, 0, 0, 1, 0],\n",
    "                                              [0, 0, 0, 0, 1]])\n",
    "predicted_covariance_estimation = np.dot(predicted_state_estimation_jacobian,\n",
    "                                         np.dot(previous_covariance_estimation, predicted_state_estimation_jacobian.T)) + Q\n",
    "```\n",
    "\n",
    "</small>\n",
    "\n",
    "In a second step, the values are updated by the sensor measurements and the external camera estimation.\n",
    "To update the values, the algorithm takes into account the uncertainty of each measurement through the R matrix. After representing the variation in the camera's position estimation, the horizontal and vertical error (RP) is estimated at +/- 1cm, and the orientation error (RP_ANGLE) at ≈1.5 degrees. Here's the code to update the prediction:\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "## Update Step      \n",
    "    if cam_got_pos and position_from_camera is not None:\n",
    "        # camera position is available\n",
    "        y = np.array([[position_from_camera[0]], [position_from_camera[1]], [position_from_camera[2]], [speed], [angular_speed]])\n",
    "        H = np.identity(5)\n",
    "        R = np.array([[RP, 0, 0, 0, 0],\n",
    "                      [0, RP, 0, 0, 0],\n",
    "                      [0, 0, RP_ANGLE, 0, 0],\n",
    "                      [0, 0, 0, r_nu_translation, 0],\n",
    "                      [0, 0, 0, 0, r_nu_rotation]]) # process noise covariance matrix\n",
    "    else:\n",
    "        # no transition, use only the speed\n",
    "        y = np.array([[speed], [angular_speed]])\n",
    "        H = np.array([[0, 0, 0, 1, 0], [0, 0, 0, 0, 1]])\n",
    "        R = np.array([[r_nu_translation, 0], [0, r_nu_rotation]]) # process noise covariance matrix\n",
    "\n",
    "    # measurement residual\n",
    "    i = y - np.dot(H, predicted_state_estimation)\n",
    "    # measurement prediction covariance\n",
    "    S = np.dot(H, np.dot(predicted_covariance_estimation, H.T)) + R\n",
    "    # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "    K = np.dot(predicted_covariance_estimation, np.dot(H.T, np.linalg.inv(S)))\n",
    "\n",
    "    # Updated state and covariance estimate\n",
    "    state_estimation = predicted_state_estimation + np.dot(K, i)    \n",
    "    P_estimation = np.dot((np.identity(5) - np.dot(K, H)), predicted_covariance_estimation)\n",
    "```\n",
    "\n",
    "</small>\n",
    "The function returns the estimated position and covariance, which will be used to calculate the prediction for the next estimation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"globnav\"></a>\n",
    "## **Global Navigation**\n",
    "\n",
    "All the global navigation function are grouped in the folder `helpers_global`.\n",
    "\n",
    "\n",
    "### A\\* computation\n",
    "Since we are working with a grid filled with 1 for global obstacles and 0 for free cells, we are using the same A\\* algorithm as the one in the exercise on Path Planning. We made a couple changes to adapt it to the project. Note that the grid computed by the vision is taking into account the size of the robot so the obstacles are bigger than in reality. After getting, the start and goal position from the camera, we can use the  `global_final` function to get the path that is already simplified by the Douglas Peucker algorithm. It also use a VISUALIZE parameter that plot the path computed on the given grid. To naviguate further from the global obstacles, we added a cost on the gScore if the cell is near an obstacle. We computed an safety margin that multiplies the current cost (the default one is 1) that is add to gScore : \n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "def calculate_safety_margin(neighbor, occupancy_grid):\n",
    "    distance_threshold_big = 5  \n",
    "    distance_threshold_small = 3\n",
    "    distance_to_obstacle = distance_to_nearest_obstacle(neighbor, occupancy_grid)\n",
    "\n",
    "    if distance_to_obstacle < distance_threshold_big:\n",
    "        if distance_to_obstacle < distance_threshold_small:\n",
    "            safety_margin = 2     \n",
    "        else: \n",
    "            safety_margin = 1.5  \n",
    "    else:\n",
    "        safety_margin = 1.0  \n",
    "\n",
    "    return safety_margin\n",
    "\n",
    "```\n",
    "\n",
    "</small>\n",
    "\n",
    "where `distance_to_nearest_obstacle` gives the minimal distance to any obstacles in the grid\n",
    "\n",
    "\n",
    "### Douglas-Peucker Algorithm\n",
    "\n",
    "Since the path given by A\\* is giving points on the grid and therefore is often changing in direction we simplified it by the Douglas Peucker Algorithm:\n",
    "\n",
    "<small>\n",
    "\n",
    "```python\n",
    "def douglas_peucker(coords, epsilon):\n",
    "    if len(coords) <= 2:\n",
    "        return [coords[0], coords[-1]]\n",
    "\n",
    "    dmax = 0\n",
    "    index = 0\n",
    "    end = len(coords) - 1\n",
    "    for i in range(1, end):\n",
    "        d = point_to_line_distance(coords[i], coords[0], coords[end])\n",
    "        if d > dmax:\n",
    "            index = i\n",
    "            dmax = d\n",
    "\n",
    "    if dmax > epsilon:\n",
    "        results1 = douglas_peucker(coords[:index + 1], epsilon)\n",
    "        results2 = douglas_peucker(coords[index:], epsilon)\n",
    "\n",
    "        results = results1[:-1] + results2\n",
    "    else:\n",
    "        results = [coords[0], coords[end]]\n",
    "\n",
    "    return results\n",
    "```\n",
    "\n",
    "</small>\n",
    "\n",
    "It is a recursive algorithm that take the path in argument and give a simplified one that only keep the point with big variation in between\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"motion\"></a>\n",
    "\n",
    "## **Motion Control**\n",
    "\n",
    "The motion control is implemented in the main `while(True)` loop in the Jupyter notebook file. \n",
    "Global navigation gives us the path saved in and the motion control has to guide the thymio through all the path checkpoints (`path[counter]`) until reaching the final goal (`path[end]`). The variable `counter` stores the current index of `path[]`that we aim for. When the robot’s position is close to it (=its position is within a circle of 4cm of radius around the current target), we increment counter. This enables us to target the next checkpoint even if a small local obstacle is placed on a checkpoint.\n",
    "\n",
    "The navigation from one checkpoint to another is done by regulating `angle_error`(=orient_robot-(angle_to_next_checkpoint)). To do so, we use a P_controller for an angle_error up to $\\pi$/6. Above this, the robot is rotated on the spot (left_motor_target=-right_motor_target). The P controller enables very smooth motion during the path.\n",
    "\n",
    "In `Thymio.py` useful functions to get the information on the robot for example` get_proximity_ground_values()`, and also functions to control the movement of the thymio, like ` motorset()`\n",
    "\n",
    "<small>\n",
    "\n",
    " ```python\n",
    "async def get_proximity_values(client):\n",
    "    # Wait for the Thymio node\n",
    "    node = await client.wait_for_node()\n",
    "    # Wait for the proximity sensor variables\n",
    "    await node.wait_for_variables({\"prox.horizontal\"})\n",
    "    # Get the proximity values : v: Stands for \"variables\" and is used to access the cached variable values.\n",
    "    proximity_values = node.v.prox.horizontal\n",
    "    return proximity_values[0:5]\n",
    "\n",
    "```\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"demo\"></a>\n",
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import opencv_jupyter_ui as jcv2\n",
    "\n",
    "## Custom Libraries \n",
    "from src.Motion_Control import thymio as th\n",
    "from src.Global_Nav import helpers_global as gn\n",
    "from src.Vision import vision as vs\n",
    "from src.Local_Nav import psymap as pm  \n",
    "from src.Local_Nav import local_navigation as ln\n",
    "from src.Filtering import filtering\n",
    "from src.Motion_Control import PID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constant \n",
    "REFRAME = True \n",
    "TS =0.01\n",
    "EPSILON_ANGLE= np.pi/10\n",
    "VISUALIZE = True\n",
    "MAP_SHAPE_MM = (1000,700)\n",
    "MAP_SHAPE_CELL = (50,35)\n",
    "ROBROAD = 80\n",
    "SIMPLIFY = 0.8\n",
    "SAVE_VIDEO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node() #_ = protected #__ = private = shouldn't access node outside of the class\n",
    "await node.lock()\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if SAVE_VIDEO:\n",
    "    videosaver = cv2.VideoWriter('VideoG16.avi',  cv2.VideoWriter_fourcc(*'MJPG'), 10, MAP_SHAPE_MM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "if REFRAME:\n",
    "    Tmap = vs.get_warp(cap,MAP_SHAPE_MM,20,10)\n",
    "\n",
    "ret,frame = cap.read()\n",
    "if ret:\n",
    "    if REFRAME:\n",
    "        frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "    fmap = vs.get_grid_fixed_map(frame,MAP_SHAPE_CELL)\n",
    "    obscont = vs.get_obstacles(frame)\n",
    "    print(\"Searching for destination...\")\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        if ret:\n",
    "            if REFRAME:\n",
    "                frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "            ret,destmm = vs.get_destination(frame)\n",
    "            if ret:\n",
    "                dest = gn.convert_to_idx([coord / 10.0 for coord in destmm],2)\n",
    "                dest[1]= 35-dest[1]\n",
    "                dest = tuple(dest)\n",
    "                break\n",
    "            else:\n",
    "                print(\"no destination !\",end='\\r')\n",
    "        else:\n",
    "            print(\"No camera !\")\n",
    "            break\n",
    "    print(\"Found destination Point at {} [mm] {} [cells]\".format(destmm,dest))\n",
    "    print(\"Searching for Robot...\")\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        if ret:\n",
    "            if REFRAME:\n",
    "                frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "            hls_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HLS_FULL) \n",
    "            ret,robpos,orient,pxpcm = vs.get_Robot_position_orientation(hls_frame)\n",
    "            if ret:\n",
    "                print(\"Robot found at {} [mm], {} [rad]\".format(robpos,orient))\n",
    "                break\n",
    "        else:\n",
    "            print(\"No camera !\")\n",
    "            break\n",
    "\n",
    "start = gn.convert_to_idx(robpos,20)\n",
    "start[1]= MAP_SHAPE_CELL[1]-start[1]\n",
    "start = tuple(start)\n",
    "path = gn.global_final(fmap,start,dest, \"8N\", VISUALIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Main boucle with Kalman Aubin\n",
    "\n",
    "local_obstacle = False\n",
    "counter=0\n",
    "cell_pos_history= []\n",
    "kidnapping_state = False\n",
    "check=[]\n",
    "check_point_prev=np.array([0,0])\n",
    "Kp = 50\n",
    "spdPID = 100\n",
    "start_time = time.time()\n",
    "\n",
    "state_estimation_prev= np.array([[robpos[0]],[robpos[1]],[orient],[0],[0]])\n",
    "P_estimation_prev =  np.diag([100, 100, 0.75, 10, 0.75])\n",
    "\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    if ret:\n",
    "\n",
    "        if REFRAME:\n",
    "            frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "\n",
    "        HLS = cv2.cvtColor(frame, cv2.COLOR_BGR2HLS_FULL)\n",
    "\n",
    "        camgotpos ,robpos,orient, scale = vs.get_Robot_position_orientation(HLS, 5)\n",
    "        if camgotpos : \n",
    "\n",
    "            robpos[1] = MAP_SHAPE_MM[1]- robpos[1]\n",
    "            orient2 = orient\n",
    "            if orient <0:\n",
    "                orient = orient +2*np.pi\n",
    "            x_est_cam = np.array([robpos[0], robpos[1], orient])\n",
    "            cell_pos_cam = np.array([x_est_cam[0]/20, x_est_cam[1]/20, orient2])\n",
    "            cell_pos_history.append(cell_pos_cam)\n",
    "\n",
    "\n",
    "        ground_values = await th.get_proximity_ground_values(client)\n",
    "        if(ground_values[0]<300 or ground_values[1]< 300):\n",
    "            print('Kidnapping detected')\n",
    "            await th.stop_motor(node)\n",
    "            kidnapping_state= True\n",
    "\n",
    "        if ground_values[0]>300 and ground_values[1]>300 and kidnapping_state and camgotpos:\n",
    "\n",
    "            kidnapping_state = False\n",
    "            start = gn.convert_to_idx(robpos,20)\n",
    "            start = tuple(start)\n",
    "            path = gn.global_final(fmap,start,dest, \"8N\", VISUALIZE)\n",
    "            state_estimation_prev = np.array([[robpos[0]],[robpos[1]], [orient], [0], [0]])\n",
    "            P_estimation_prev =  np.diag([100, 100, 0.75, 10, 0.75])\n",
    "            counter =0\n",
    "            check_point_prev=np.array([0,0])\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "        if x_est_cam is not None:\n",
    "            if abs(state_estimation_prev[2][0]-x_est_cam[2]) > 1:\n",
    "                x_est_cam = None\n",
    "        state_estimation, P_estimation, speed, angular_speed, start_time, angle = await filtering.get_position(state_estimation_prev, P_estimation_prev, start_time,camgotpos,x_est_cam, node )\n",
    "        state_estimation_prev = state_estimation\n",
    "        P_estimation_prev = P_estimation\n",
    "\n",
    "        position = np.array([state_estimation[0].item(), state_estimation[1].item()])\n",
    "        theta = angle\n",
    "        position = position / 20.0\n",
    "        position_array = np.array(position)\n",
    "\n",
    "        state_history = np.array([position_array[0], position_array[1], theta[0]])\n",
    "        check.append(state_history)\n",
    "\n",
    "        check_point, counter = gn.next_checkpoint2(path, position, counter,local_obstacle)\n",
    "            \n",
    "        if not kidnapping_state:\n",
    "            if np.any(check_point_prev != check_point):\n",
    "                print(f\"robot at {position}, grid coord {gn.convert_to_idx(position, 1)} next checkpoint at{check_point}\")\n",
    "                check_point_prev = check_point\n",
    "                \n",
    "            if abs(position[0]-path[-1][0])<1 and abs(position[1]-path[-1][1])<1:\n",
    "                await th.stop_motor(node)\n",
    "                break\n",
    "\n",
    "            \n",
    "            angle_error=  theta-th.compute_angle(gn.convert_to_idx(position,1), path[counter])\n",
    "            if angle_error > np.pi :\n",
    "                angle_error = angle_error-2*np.pi\n",
    "            if angle_error < -np.pi:\n",
    "                angle_error = angle_error+ 2*np.pi\n",
    "\n",
    "            \n",
    "            capthall = pm.hallucinate_map([position[0],position[1],(-orient)],obscont)\n",
    "            sens = await th.get_proximity_values(client)\n",
    "            if (sum(sens[i] > 1000 for i in range(0, 5)) > 0):\n",
    "                local_obstacle = True\n",
    "\n",
    "            if(local_obstacle):\n",
    "                print(\"Entering Local navigation mode\")\n",
    "                await ln.local_navigation(client,node,[position[0],position[1],(-orient)],obscont)\n",
    "                \n",
    "                if(not sum(sens[i] > 1000 for i in range(0, 5)) > 0):\n",
    "                    await th.motorset(node,100,100)\n",
    "                    time.sleep(1.5)\n",
    "                    local_obstacle = False\n",
    "            #motor control\n",
    "            else :\n",
    "                if(angle_error>EPSILON_ANGLE):\n",
    "                    await th.motorset(node,70,-70)\n",
    "                elif (angle_error<-EPSILON_ANGLE):\n",
    "                    await th.motorset(node,-70,70)\n",
    "                else:\n",
    "                    speed_l, speed_r = PID.PIDController(Kp,spdPID, angle_error)\n",
    "                    await th.motorset(node,speed_l,speed_r)\n",
    "                    \n",
    "\n",
    "        if VISUALIZE:\n",
    "            vizu = vs.visualizer(HLS)\n",
    "            omap =vs.grid_fixedmap_visualizer(fmap.transpose(),MAP_SHAPE_MM)\n",
    "            obsimg = cv2.merge([omap,omap,omap])\n",
    "            vizu = cv2.bitwise_or(vizu,obsimg)\n",
    "            vizu = vs.draw_obstacles_poly(vizu,obscont,(255,255,0),2)\n",
    "            vizu = cv2.circle(vizu,destmm,20,(50,25,100),4)\n",
    "            vizu = cv2.addWeighted(vizu,0.5,frame,0.5,0)\n",
    "            vizu = vs.show_path(vizu,path,20,10)\n",
    "            vizu = vs.show_Kalman(vizu,state_estimation,P_estimation,10)\n",
    "            robpos[1] = MAP_SHAPE_MM[1] - robpos[1]\n",
    "            vizu = vs.paint_robot(vizu,(0,0,200),robpos,orient,pxpcm)\n",
    "            vizu = pm.hallucinate_map([robpos[0],robpos[1],(-orient)],obscont,vizu)\n",
    "            videosaver.write(vizu) \n",
    "            jcv2.imshow(\"Map\",vizu)\n",
    "            if jcv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                await th.stop_motor(node)\n",
    "                break\n",
    "        else:\n",
    "            print(\"###################################################\",end='\\r')\n",
    "            print(\"pos: {},{:.2f} dest: {} fhal: {}\".format(robpos,orient,destmm,capthall),end='\\r')\n",
    "    \n",
    "    else :\n",
    "        print(\"error : camera failure.\")\n",
    "        break\n",
    "\n",
    "videosaver.release()\n",
    "jcv2.destroyAllWindows()\n",
    "await th.stop_motor(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_values = [coord[0] for coord in check]\n",
    "y_values = [coord[1] for coord in check]\n",
    "x_path =   [coord[0] for coord in path]\n",
    "y_path =  [coord[1] for coord in path]\n",
    "\n",
    "# Tracer le graphique x en fonction de y\n",
    "plt.plot(x_values, y_values, marker='.', linestyle='-')\n",
    "plt.plot(x_path, y_path, marker ='o', color = 'red')\n",
    "plt.axis('equal')\n",
    "plt.title('Historique de position')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await th.stop_motor(node)\n",
    "node.unlock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "## **Results**\n",
    "\n",
    "To conclude, the code seems to work well, the thymio reacts well to local obstacles and reaches the Goal in a smooth way, when a path exists. The only case that we did not consider is when the only path possible is blocked by local obstacles, the robot doesn’t stop and may pass over global obstacles. We struggled a lot to put all the work together and spend most of the time doing this. The filter was also hard to implement, but worked at the end and gives good approximation when the camera is hidden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MobileRobotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

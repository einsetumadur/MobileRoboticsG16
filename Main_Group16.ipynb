{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "r { color: Red }\n",
    "o { color: Orange }\n",
    "g { color: Green }\n",
    "b { color: LightBlue}\n",
    "w { color: White}\n",
    "</style>\n",
    "\n",
    "# **Mobile Robotics Presentation Group 16**\n",
    "EPFL Course MICRO-452, fall 2023\n",
    "- Gilles Regamey 296642\n",
    "- Julien Droulet SCIPER\n",
    "- Tom Rathjen    SCIPER\n",
    "- Aubin Sabatier SCIPER\n",
    "\n",
    "## Table of content\n",
    "-   [Choices](#choices)\n",
    "    - environement\n",
    "    - filter\n",
    "    - navigation\n",
    "-   [Modules](#modules)\n",
    "    - Vision\n",
    "    - Global Navigation\n",
    "    - Local Navigation\n",
    "    - Filtering\n",
    "    - Motion Control\n",
    "-   [Demonstration](#demo)\n",
    "    - setup\n",
    "    - loop\n",
    "-   [Results](#results)\n",
    "    - encountered problems\n",
    "    - performance\n",
    "    - improvements\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"choices\"></a>\n",
    "# Choices taken\n",
    "The project called for a environement map to navigate with fixed and un-planned obstacles to a destination point from any point. Fixed obstacles are constant trough the navigation and the robot can't use its sensor to detect them, they are only given by the camera at the start. Un-planned obstacles are detected only by the robot and can move at any time during navigation. \n",
    "\n",
    "## Environement\n",
    "For the sake of simplicity, our map is a white A0 paper sheet with 4 <b>blue</b> box (4x4cm) at the corners. The Fixed obstacles are black pieces of cardboard of any shape. Mobile obstacles are <w>white</w> boxes of 4x4x4 cm 3d printed in PLA to have a more consistent reading with the proximity sensors. The robot is fitted with a paper hat with <r>red</r> circles in a isosceles triangle pattern for position and orientation. The destination is a big green circle 8cm in diameter. The image of the map is reframed to have a resolution of $1 \\frac{px}{mm}$.\n",
    "\n",
    "Below a theoretical map image before processing.\n",
    "\n",
    "<img src=\"src\\Vision\\test_data\\test_map.png\" />\n",
    "\n",
    "The fixed obstacles are abstracted in two ways for two different purposes:\n",
    "- For Global navigation: occupancy grid of constant size.\n",
    "- For Local navigation: polygonal contour approximation.\n",
    "We'll see why in the modules descriptions.  \n",
    "\n",
    "## Filtering\n",
    "The camera gives us a pretty good estimation for the position and orientation of the robot, but we need to be able to navigate without it as somes frames can extract the position because of the changes in illumination or simply if the robot can't be seen. \n",
    "\n",
    "<r>**WHY A KALMAN ? - #[TODO]AUBIN**</r>\n",
    "\n",
    "## Global navigation\n",
    "For the path planning we choose the A\\* Algorithm with the occupancy grid given by the vision module.\n",
    "To have a margin to the obtacle \n",
    "\n",
    "<r>**why A\\* - #[TODO]JULIEN**</r>\n",
    "\n",
    "## Local navigation\n",
    "Mobile and un-planned obstacles have to be detected and avoided. We choose to use a neural network system to be more robust to non standard obstacles. \n",
    "\n",
    "<r>**why neural #[TODO] TOM**</r>\n",
    "\n",
    "<a id=\"modules\"></a>\n",
    "# Modules description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vision**\n",
    "The vision module is responsible for everything concerning the camera and vizualization.\n",
    "Most importantly it has to detect the robot, the obstacles, the destination and the map correctly.\n",
    "Visualization functions have been more of a tool to see and debug outputs of detection.\n",
    "As our map elements are color based, we use the HLS (Hue Luminosity Saturation) encoding of the image to have a better chance at detecting color patches.\n",
    "We use the opencv library extensively because of the blazingly fast parallel computation and all the usefull tools for detection and image processing.\n",
    "\n",
    "### map detection\n",
    "In the setup phase, we need to calibrate the camera to reframe the map in a consitant way to give a baseline for other functions. Opencv has a very usefull function called `warpPerspective()` just for that, we just need to give it the <w>warp matrix</w> and desired output shape. The function `vision.get_warp()` gives us this <w>warp matrix</w> from the image using the algorithm below and with the help of smaller functions called `reorder_border()`, `blob_point_list()` and another opencv function to compute the matrix from reference points `getPerspectiveTransform`. Note that this is not functionnal code, the real function and subsequent vision function can be found in `/src/vision.py`.\n",
    "\n",
    "```python\n",
    "def get_warp(capture_stream,ROI,padding,number_of_samples):\n",
    "    while valid_samples < number_of_samples:\n",
    "        frame = capture_stream.read()\n",
    "        hsl_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HLS_FULL)\n",
    "        nb_points,point_list = blob_point_list(Blue,CORNER_BLOB_FILT,CORNER_BLOB_VAL)\n",
    "        if nb_points == 4:\n",
    "            isvalid,ord_list = reorder_border(point_list)\n",
    "            if isvalid:\n",
    "                corners[:,:,smp] = ord_list\n",
    "                valid_samples += 1\n",
    "    avgCorn = mean(corners) \n",
    "     \n",
    "    return cv2.getPerspectiveTransform(avgCorn,Sheetpts)\n",
    "```\n",
    "The `reorder_border()` function is just ordering the points relative to the center of the polygon they form (above/under,left/right) in order to give a consistant list of point for the averaging process.\n",
    "\n",
    "Note that no action is taken for tangential and radial distortions. Opencv provides function for camera calibration and undistortion methods that need a calibration pattern (Charuco boards). We tried to use it but coulnd't get consitant camera matrix, it just made things worse most of the time so we decided to drop the idea knowing that robot position detection was good enough at our scale.\n",
    "\n",
    "### Obstacle detection and abstraction\n",
    "\n",
    "For the grid map of the obstacles we just apply a threshold to a scaled down filtered grayscale image of the map using opencv functions. You can also add a safe radius around obstacles using dilatation. \n",
    "\n",
    "```python\n",
    "def get_grid_fixed_map(frame,shape,tresh=50,kernsz=5,robrad=80):\n",
    "    kernel = np.ones((kernsz,kernsz),np.uint8)\n",
    "    pxmap = cv2.inRange(frame,(0,0,0),(tresh,tresh,tresh))\n",
    "    pxmap = cv2.morphologyEx(pxmap,cv2.MORPH_OPEN,kernel)\n",
    "    if robrad != 0:\n",
    "        safecircle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(robrad,robrad))\n",
    "        pxmap = cv2.dilate(pxmap,safecircle)\n",
    "    temp = cv2.resize(pxmap, shape, interpolation=cv2.INTER_LINEAR)\n",
    "    _,output = cv2.threshold(temp,10,1,type=cv2.THRESH_BINARY)\n",
    "```\n",
    "\n",
    "For the polygonal abstraction of the obstacles, we use opencv `findContours()` and `approxPolyDP()` to get an array of obstacles described as arrays of corner points.\n",
    "\n",
    "\n",
    "```python\n",
    "def get_obstacles(frame,tresh=50,eps=10,robrad=0):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    pxmap = cv2.inRange(frame,(0,0,0),(tresh,tresh,tresh))\n",
    "    pxmap = cv2.morphologyEx(pxmap,cv2.MORPH_OPEN,kernel)\n",
    "    if robrad != 0:\n",
    "        safecircle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(robrad,robrad))\n",
    "        pxmap = cv2.dilate(pxmap,safecircle)\n",
    "    contp,hier =  cv2.findContours(pxmap,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    obstacles = []\n",
    "    for cont in contp:\n",
    "        cont = np.array(cont,dtype=np.float32).reshape((-1,2))\n",
    "        scont = cv2.approxPolyDP(cont,eps,True)\n",
    "        obstacles.append(scont)\n",
    "    return obstacles\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Module**\n",
    "\n",
    "### submodule 1\n",
    "description of module \n",
    "\n",
    "equation inline : $\\alpha + \\beta = e$\n",
    "\n",
    "equations by line\n",
    "\\begin{equation}\n",
    "S(\\omega)=1.466\\, H_s^2 \\frac{\\omega_0^5}{\\omega^6} \\exp\\Bigl[-3^{\\frac{\\omega}{\\omega_0}}\\Bigr]^2\n",
    "\\end{equation}\n",
    "\n",
    "### submodule 2\n",
    "description of module \n",
    "code implementation\n",
    "```python\n",
    "if (goodboy):\n",
    "    reward += 1\n",
    "else:\n",
    "    reward = 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"demo\"></a>\n",
    "# Demontration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import opencv_jupyter_ui as jcv2\n",
    "\n",
    "## Custom Libraries \n",
    "from src.Motion_Control import thymio as th\n",
    "from src.Global_Nav import helpers_global as gn\n",
    "from src.Vision import vision as vs\n",
    "from src.Local_Nav import psymap as pm  \n",
    "from src.Local_Nav import local_navigation as ln\n",
    "from src.Filtering import filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constant \n",
    "REFRAME = True \n",
    "TS =0.01\n",
    "EPSILON_ANGLE= np.pi/10\n",
    "VISUALIZE = True\n",
    "MAP_SHAPE_MM = (1000,700)\n",
    "MAP_SHAPE_CELL = (50,35)\n",
    "ROBROAD = 80\n",
    "SIMPLIFY = 0.8\n",
    "SAVE_VIDEO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node() #_ = protected #__ = private = shouldn't access node outside of the class\n",
    "await node.lock()\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if SAVE_VIDEO:\n",
    "    videosaver = cv2.VideoWriter('VideoG16.avi',  cv2.VideoWriter_fourcc(*'MJPG'), 18, MAP_SHAPE_MM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "if REFRAME:\n",
    "    Tmap = vs.get_warp(cap,MAP_SHAPE_MM,20,10)\n",
    "\n",
    "ret,frame = cap.read()\n",
    "if ret:\n",
    "    if REFRAME:\n",
    "        frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "    fmap = vs.get_grid_fixed_map(frame,MAP_SHAPE_CELL)\n",
    "    obscont = vs.get_obstacles(frame)\n",
    "    print(\"Searching for destination...\")\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        if ret:\n",
    "            if REFRAME:\n",
    "                frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "            ret,destmm = vs.get_destination(frame)\n",
    "            if ret:\n",
    "                dest = gn.convert_to_idx([coord / 10.0 for coord in destmm],2)\n",
    "                dest[1]= 35-dest[1]\n",
    "                dest = tuple(dest)\n",
    "                break\n",
    "            else:\n",
    "                cv2.imshow()\n",
    "        else:\n",
    "            print(\"No camera !\")\n",
    "            break\n",
    "    print(\"Found destination Point at {} [mm] {} [cells]\".format(destmm,dest))\n",
    "    print(\"Searching for Robot...\")\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        if ret:\n",
    "            if REFRAME:\n",
    "                frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "            hls_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HLS_FULL) \n",
    "            ret,robpos,orient,pxpcm = vs.get_Robot_position_orientation(hls_frame)\n",
    "            if ret:\n",
    "                print(\"Robot found at {} [mm], {} [rad]\".format(robpos,orient))\n",
    "                break\n",
    "        else:\n",
    "            print(\"No camera !\")\n",
    "            break\n",
    "\n",
    "start = gn.convert_to_idx(robpos,20)\n",
    "start[1]= MAP_SHAPE_CELL[1]-start[1]\n",
    "start = tuple(start)\n",
    "path = gn.global_final(fmap,start,dest, \"8N\", VISUALIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Main boucle with Kalman Aubin\n",
    "\n",
    "local_obstacle = False\n",
    "counter=0\n",
    "cell_pos_history= []\n",
    "\n",
    "check=[]\n",
    "check_point_prev=np.array([0,0])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "state_estimation_prev= np.array([[robpos[0]],[robpos[1]],[orient],[0],[0]])\n",
    "P_estimation_prev =  np.diag([100, 100, 0.75, 10, 0.75])\n",
    "\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    if ret:\n",
    "\n",
    "        if REFRAME:\n",
    "            frame = cv2.warpPerspective(frame,Tmap,MAP_SHAPE_MM)\n",
    "\n",
    "        HLS = cv2.cvtColor(frame, cv2.COLOR_BGR2HLS_FULL)\n",
    "\n",
    "        camgotpos ,robpos,orient, scale = vs.get_Robot_position_orientation(HLS, 5)\n",
    "        if camgotpos : \n",
    "\n",
    "            robpos[1] = MAP_SHAPE_MM[1]- robpos[1]\n",
    "            orient2 = orient\n",
    "            if orient <0:\n",
    "                orient = orient +2*np.pi\n",
    "            x_est_cam = np.array([robpos[0], robpos[1], orient])\n",
    "            cell_pos_cam = np.array([x_est_cam[0]/20, x_est_cam[1]/20, orient2])\n",
    "            cell_pos_history.append(cell_pos_cam)\n",
    "\n",
    "        state_estimation, P_estimation, speed, angular_speed, start_time,theta = await filtering.get_position(state_estimation_prev, P_estimation_prev, start_time,camgotpos,x_est_cam, node )\n",
    "        state_estimation_prev = state_estimation\n",
    "        P_estimation_prev = P_estimation\n",
    "\n",
    "        position = np.array([state_estimation[0].item(), state_estimation[1].item()])\n",
    "        position = position / 20.0\n",
    "        position_array = np.array(position)\n",
    "\n",
    "        check_point, counter = gn.next_checkpoint2(path, position, counter,local_obstacle)\n",
    "        # if theta > np.pi :\n",
    "            # theta = theta-2*np.pi\n",
    "        # if theta < -np.pi:\n",
    "            #  theta = theta+ 2*np.pi    \n",
    "            \n",
    "        state_history = np.array([position_array[0], position_array[1], theta[0]])\n",
    "        check.append(state_history)\n",
    "\n",
    "        if np.any(check_point_prev != check_point):\n",
    "            print(f\"robot at {position}, grid coord {gn.convert_to_idx(position, 1)} next checkpoint at{check_point}\")\n",
    "            check_point_prev = check_point\n",
    "            \n",
    "        if abs(position[0]-path[-1][0])<1 and abs(position[1]-path[-1][1])<1:\n",
    "            await th.stop_motor(node)\n",
    "            break\n",
    "\n",
    "        \n",
    "        angle_error=  theta-th.compute_angle(gn.convert_to_idx(position,1), path[counter])\n",
    "        if angle_error > np.pi :\n",
    "            angle_error = angle_error-2*np.pi\n",
    "        if angle_error < -np.pi:\n",
    "            angle_error = angle_error+ 2*np.pi\n",
    "\n",
    "        \n",
    "        capthall = pm.hallucinate_map([position[0],position[1],(-orient)],obscont)\n",
    "        sens = await th.get_proximity_values(client)\n",
    "        if (sum(sens[i] > 1000 for i in range(0, 5)) > 0):\n",
    "            local_obstacle = True\n",
    "\n",
    "        if(local_obstacle):\n",
    "            print(\"Entering Local navigation mode\")\n",
    "            await ln.local_navigation2(client,node,[position[0],position[1],(-orient)],obscont)\n",
    "            \n",
    "            if(not sum(sens[i] > 1000 for i in range(0, 5)) > 0):\n",
    "                await th.motorset(node,100,100)\n",
    "                time.sleep(1.5)\n",
    "                local_obstacle = False\n",
    "        #motor control\n",
    "        else :\n",
    "            if(angle_error>EPSILON_ANGLE):\n",
    "                await th.motorset(node,70,-70)\n",
    "            elif (angle_error<-EPSILON_ANGLE):\n",
    "                await th.motorset(node,-70,70)\n",
    "            else:\n",
    "                await th.motorset(node,120,120)\n",
    "                 \n",
    "\n",
    "        if VISUALIZE:\n",
    "            vizu = vs.visualizer(HLS)\n",
    "            omap =vs.grid_fixedmap_visualizer(fmap.transpose(),MAP_SHAPE_MM)\n",
    "            obsimg = cv2.merge([omap,omap,omap])\n",
    "            vizu = cv2.bitwise_or(vizu,obsimg)\n",
    "            vizu = vs.draw_obstacles_poly(vizu,obscont,(255,255,0),2)\n",
    "            vizu = cv2.circle(vizu,destmm,20,(50,25,100),4)\n",
    "            vizu = cv2.addWeighted(vizu,0.5,frame,0.5,0)\n",
    "            vizu = vs.show_path(vizu,path,20,10)\n",
    "            vizu = vs.show_Kalman(vizu,state_estimation,P_estimation,10)\n",
    "            robpos[1] = MAP_SHAPE_MM[1] - robpos[1]\n",
    "            vizu = vs.paint_robot(vizu,(0,0,200),robpos,orient,pxpcm)\n",
    "            vizu = pm.hallucinate_map([robpos[0],robpos[1],(-orient)],obscont,vizu)\n",
    "            videosaver.write(vizu) \n",
    "            jcv2.imshow(\"Map\",vizu)\n",
    "            if jcv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                await th.stop_motor(node)\n",
    "                break\n",
    "        else:\n",
    "            print(\"###################################################\",end='\\r')\n",
    "            print(\"pos: {},{:.2f} dest: {} fhal: {}\".format(robpos,orient,destmm,capthall),end='\\r')\n",
    "    \n",
    "    else :\n",
    "        print(\"error : camera failure.\")\n",
    "        break\n",
    "\n",
    "videosaver.release()\n",
    "jcv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await th.stop_motor(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_values = [coord[0] for coord in check]\n",
    "y_values = [coord[1] for coord in check]\n",
    "x_path =   [coord[0] for coord in path]\n",
    "y_path =  [coord[1] for coord in path]\n",
    "\n",
    "# Tracer le graphique x en fonction de y\n",
    "plt.plot(x_values, y_values, marker='.', linestyle='-')\n",
    "plt.plot(x_path, y_path, marker ='o', color = 'red')\n",
    "plt.axis('equal')\n",
    "plt.title('Historique de position')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MobileRobotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
